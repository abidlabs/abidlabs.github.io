---
layout: post
published: true
comments: true
permalink: /:categories/:year/:month/:day/
categories: [journal]
---

If you have an imbalanced dataset, the typical strategies to train a classifier are to oversample the minority class, or modify the loss function to penalize mis-classifications of the minority class more than mis-classifications of the majority class. If you think about it, these methods are mathematically equivalents?

Based on experiments that [I ran with synthetic data](https://github.com/abidlabs/AtomsOfDeepLearning/blob/master/20%20Atomic%20Experiments%20in%20Deep%20Learning.ipynb), these methods are not very effective either. Yet they are widely used. Am I missing something?

---
layout: post
title: 02 Mar 2018
published: true
comments: true
permalink: /:categories/:year/:month/:day/
categories: [journal]
---

David Tse recently gave a talk where there is often an unexplored information-theoretic angle to many well-studied machine learning problems. For example, using the maximum entropy principle to fit a probability distribution based on samples from it, versus using maximum likelihood.

That thought ran through my head as I was reading this new paper for journal club: the authors use mutual information as the criterion to determine which features of an image (or input feature) are the most responsible for a prediction that is made by a trained neural network: https://arxiv.org/pdf/1802.07814.pdf
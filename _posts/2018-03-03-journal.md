---
layout: post
title: 02 Mar 2018
published: true
comments: true
permalink: /:categories/:year/:month/:day/
categories: [journal]
---

David Tse recently gave a talk where he said that there is often an unexplored information-theoretic angle to many well-studied machine learning problems. For example, you can use the maximum entropy principle to fit a probability distribution based on samples from it, instead ofs using maximum likelihood.

That thought ran through my head as I was reading this new paper for journal club: the authors use mutual information as the criterion to determine which features of an image (or any input more generally) are the most responsible for a prediction that is made by a trained neural network: [https://arxiv.org/pdf/1802.07814.pdf](https://arxiv.org/pdf/1802.07814.pdf)
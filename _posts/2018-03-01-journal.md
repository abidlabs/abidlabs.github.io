---
layout: post
title: 01 March 2018 - A Mathless Introduction to The EM Algorithm
published: false
permalink: /:categories/:year/:month/:day/
categories: [journal]
---

Today, I spent some understanding the Expectation-Maximization (EM) algorithm. It was the 8th or 9th time I had tried to understand it, because even though there are many nice tutorials about it online, about three quarters of the way through reading a tutorial, my eyes start to glaze over the math, and I end up leaving with only a "high-level understanding" of it. Today, I think I truly get it, and I'm going to try to explain it in a way that would have been clearest to me from the beginning, with a minimal amount of mathematical notation until the very end. This is divided into "Three Passes", with each pass understanding the algorithm at a deeper.

**Motivating problem**: suppose you have two friends, Ali and Bilal, each of whom pick a random real number (\\(A\\) and \\(B\\)) between 0 and 1, and write their number on many slips of paper. Except they don't write their number exactly, but write it down a "noisy" version of it (meaning they add or subtract a value, picked from zero-mean normal distribution with possibly different variances, before writing it down). After Ali and Bilal write down their numbers, they stuff the slips into an envelope and hand it to you. Based only on the numbers written on the slips of paper, can you figure out the original pair of numbers \\(\\{A,B\\}\\)? (Which number was picked by whom doesn't matter.)

**Example:** The slips of paper read: 0.05, 0.07, 0.48, 0.06, 0.08, 0.53, 0.07, 0.03, 0.12, 0.45, 0.11, 0.47, 0.06, 0.9

**First Pass: Intuition**: 

(1) If there was only one person, say Ali, writing down numbers, then this would be a very simple question. The best strategy would be to simply average the numbers that you see in the envelope. A little bit of math, or just intuition, will tell you that this will give you a good estimate of \\(A\\). In fact, this estimate is _consistent_, meaning that with enough slips of paper, you can estimate \\(A\\) to any desired degree of accuracy. 

(2) The tricky thing in this problem is that you have two real numbers \\(A\\) and \\(B\\), and you don't know whether the slip of paper is a noisy version of \\(A\\) or \\(B\\). In other words, in addition to the unknown noise, there is an unknown variable, which corresponds to the assignment of the slips to either Ali or Bilal. If you knew this assignment (if they each handed you a separate envelope), then the problem would be much easier. (This is a Hallmark of EM problems: if you knew some additional information, the estimate would be super easy.)

(3) But all hope is not lost. Looking at the example above, it seems kind of "obvious" that the numbers form two clusters, one around 0.06, and another around 0.5. Maybe  
- We could make this assumption and then calculate the means
- However, what if we're wrong? Maybe instead, both $A$ and $B$ are close to 0.5 and $A$ chooses a normal distribution with a higher variance
- How do we know which one? For each guess, we can always calculate the compute the probability that our data would be generated under that assumption, and choose the guess that has the highest probability. This is called likelihood, and this method of choosing the highest is known as the maximum likelihood estimator.
- This seems like a good idea, but the problem is that there are an infinite number of values for $A$ and $B$ and the variances of the normal distributions. How could we efficiently search through all of them?
- So far, we have been guessing the mean and variances. What seems like a better idea is to guess an assignment of slips to Ali and Bilal. If there are $N$ slips, there are $2^N$ slips, then we can easily compute the optimal $A$ and $B$. 
- But this is still exponential. Can we do better? What if we guess a
 
This process of making a guess and improving

So we basically have two questions at this stage:
* Is this process of iterative improvement guaranteed to improve the likelihood? Could we end up assigning the labels and recalculating $A$ and $B$ (Local optimum)
* Is this guaranteed to reach the estimate with the highest estimate? (Global optimum)

**Second Pass: Local Optimum**: 

We answer these questions in the Second Pass and Third Pass respectively.

**Third Pass: Global Optimum**: 

In other words, is the local optimum the same as the global optimum. This is a very common question that is usually answered by looking at the concavity of a function (for our purposes, a concave function is one that only has one peak, like a mountain surrounded by a valley, as opposed to many, like a mountain surrounded by hills). If we inch up a concave function, we are guaranteed to reach its global maximum, but this is not true for convex functions. As you may remember from calculus, you can guarantee that a convex function is 

Is there a function here whose convexity you think is related

** Let's fill in the math **:


** **  
---
layout: post
title: Gently Building Up The EM Algorithm
published: false
---

Today, I spent some understanding the Expectation-Maximization (EM) algorithm. It was the 8th or 9th time I had tried to understand it, because even though there are many nice tutorials about it online, about three quarters of the way through reading a tutorial, my eyes start to glaze over the math, and I end up leaving with only a "high-level understanding" of it. Today, I think I truly get it, and I'm going to try to explain it in a way that would have been clearest to me from the beginning. I've divided this article into _3 passes_, each of which provide an understanding of the algorithm from a different perspective.

# Pass 1: Intuition 

Suppose you have two friends, Ali and Bilal, each of whom have a biased coin that flips heads with probability (\\(a\\) and \\(b\\)) respectively. They flip their coins 10 times and write down the number of heads and tails they get on a slip of paper. They repeat this process several times, generating producing many slips of papers, which they stuff into an envelope and hand it to you. Based only on the numbers written on the slips of paper, can you figure out the original probabilities \\(\\{a,b\\}\\)? (The order doesn't matter.)

{% include image.html name="flips.png" caption="Here's an example of the slips of paper with the number of heads (H) and tails (T)" width="64%"%}

(1) If there was only one person, say Ali, writing down numbers, then this would be a very simple question. The best strategy would be to simply average the numbers that you see in the envelope. A little bit of math, or just intuition, will tell you that this will give you a good estimate of \\(A\\). In fact, this estimate is _consistent_, meaning that with enough slips of paper, you can estimate \\(A\\) to any desired degree of accuracy. 

(2) The tricky thing in this problem is that you have two real numbers \\(A\\) and \\(B\\), and you don't know whether the slip of paper is a noisy version of \\(A\\) or \\(B\\). In other words, in addition to the unknown noise, there is an unknown variable, which corresponds to the assignment of the slips to either Ali or Bilal. If you knew this assignment (if they each handed you a separate envelope), then the problem would be much easier. (This is a Hallmark of EM problems: if you knew some additional information, the estimate would be super easy.)

(3) But all hope is not lost. Looking at the example above, it seems kind of "obvious" that the numbers form two clusters, one around 0.06, and another around 0.5. Maybe  
- We could make this assumption and then calculate the means
- However, what if we're wrong? Maybe instead, both $A$ and $B$ are close to 0.5 and $A$ chooses a normal distribution with a higher variance
- How do we know which one? For each guess, we can always calculate the compute the probability that our data would be generated under that assumption, and choose the guess that has the highest probability. This is called likelihood, and this method of choosing the highest is known as the maximum likelihood estimator.
- This seems like a good idea, but the problem is that there are an infinite number of values for $A$ and $B$ and the variances of the normal distributions. How could we efficiently search through all of them?
- So far, we have been guessing the mean and variances. What seems like a better idea is to guess an assignment of slips to Ali and Bilal. If there are $N$ slips, there are $2^N$ slips, then we can easily compute the optimal $A$ and $B$. 
- But this is still exponential. Can we do better? What if we guess a
 
This process of making a guess and improving

So we basically have two questions at this stage:
* Is this process of iterative improvement guaranteed to improve the likelihood? Could we end up assigning the labels and recalculating $A$ and $B$ (Local optimum)
* Is this guaranteed to reach the estimate with the highest estimate? (Global optimum)

We answer these questions in the Second Pass and Third Pass respectively.

**Second Pass: Math**: 


**Third Pass: Simulation**: 

In other words, is the local optimum the same as the global optimum. This is a very common question that is usually answered by looking at the concavity of a function (for our purposes, a concave function is one that only has one peak, like a mountain surrounded by a valley, as opposed to many, like a mountain surrounded by hills). If we inch up a concave function, we are guaranteed to reach its global maximum, but this is not true for convex functions. As you may remember from calculus, you can guarantee that a convex function is 

Is there a function here whose convexity you think is related

** Let's fill in the math **:


** **  